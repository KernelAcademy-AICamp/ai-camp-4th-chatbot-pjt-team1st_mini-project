"""
ğŸ¤– llm_service.py - AI ì„œë¹„ìŠ¤
==============================

Google Gemini API ì—°ë™ ë¡œì§ì…ë‹ˆë‹¤.
"""

import json
import re
import os
from config.prompts import (
    SYSTEM_PROMPT,
    ARTIFACT_CONTEXT,
    QUIZ_PROMPT,
    MESSAGES
)
from config.settings import AI_CONFIG


class LLMService:
    """Google Gemini API ì—°ë™ ì„œë¹„ìŠ¤"""

    def __init__(self, api_key: str = None):
        self.api_key = api_key
        self.client = None
        self.model = None

        if api_key:
            try:
                import google.generativeai as genai
                genai.configure(api_key=api_key)
                self.model = genai.GenerativeModel('gemini-2.0-flash')
                self.client = True  # í´ë¼ì´ì–¸íŠ¸ í™œì„±í™” í‘œì‹œ
                print("âœ… Gemini API ì—°ê²°ë¨")
            except ImportError:
                print("âš ï¸ google-generativeai íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install google-generativeai")
            except Exception as e:
                print(f"âš ï¸ Gemini ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")

    def build_system_prompt(self, artifact: dict = None) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""

        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
        system_prompt = SYSTEM_PROMPT

        # ìœ ë¬¼ ì •ë³´ ì¶”ê°€
        if artifact:
            artifact_context = ARTIFACT_CONTEXT.format(
                name=artifact.get("name", ""),
                name_en=artifact.get("name_en", ""),
                period=artifact.get("period", ""),
                material=artifact.get("material", ""),
                location=artifact.get("location", ""),
                description=artifact.get("description", ""),
                fun_facts=", ".join(artifact.get("fun_facts", []))
            )
            system_prompt += artifact_context

        return system_prompt

    def chat(self, user_message: str, artifact: dict = None) -> str:
        """LLMê³¼ ëŒ€í™”"""

        system_prompt = self.build_system_prompt(artifact)

        # Gemini API í˜¸ì¶œ
        if self.model:
            try:
                full_prompt = f"{system_prompt}\n\nì‚¬ìš©ì: {user_message}"
                response = self.model.generate_content(full_prompt)
                return response.text
            except Exception as e:
                return f"API ì˜¤ë¥˜: {str(e)}"

        # API ì—†ìœ¼ë©´ ê¸°ë³¸ ì‘ë‹µ
        return self._fallback_response(user_message, artifact)

    def generate_quiz(self, artifact: dict) -> dict:
        """í€´ì¦ˆ ìƒì„±"""

        if self.model:
            try:
                prompt = QUIZ_PROMPT.format(
                    artifact_name=artifact["name"]
                )

                response = self.model.generate_content(prompt)

                # JSON íŒŒì‹±
                response_text = response.text
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group())
            except Exception as e:
                print(f"í€´ì¦ˆ ìƒì„± ì˜¤ë¥˜: {e}")

        # í´ë°±: ê¸°ë³¸ í€´ì¦ˆ
        return self._fallback_quiz(artifact)

    def _fallback_response(self, user_message: str, artifact: dict = None) -> str:
        """API ì—†ì„ ë•Œ ê¸°ë³¸ ì‘ë‹µ"""

        if artifact:
            return f"""## {artifact['name']}

**ì‹œëŒ€**: {artifact['period']}
**ì¬ë£Œ**: {artifact['material']}
**ìœ„ì¹˜**: {artifact['location']}

{artifact['description']}

ğŸ’¡ **ì•Œê³  ê³„ì…¨ë‚˜ìš”?**
{chr(10).join('â€¢ ' + fact for fact in artifact.get('fun_facts', []))}

ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì§ˆë¬¸í•´ ì£¼ì„¸ìš”! "í€´ì¦ˆ"ë¼ê³  ì…ë ¥í•˜ë©´ í€´ì¦ˆë¥¼ í’€ ìˆ˜ ìˆì–´ìš” ğŸ¯"""

        return MESSAGES["artifact_not_found"]

    def _fallback_quiz(self, artifact: dict) -> dict:
        """ê¸°ë³¸ í€´ì¦ˆ (API ì—†ì„ ë•Œ)"""

        return {
            "question": f"'{artifact['name']}'ì€(ëŠ”) ì–¸ì œ ë§Œë“¤ì–´ì¡Œë‚˜ìš”?",
            "options": ["ì‚¼êµ­ì‹œëŒ€", "ê³ ë ¤ì‹œëŒ€", "ì¡°ì„ ì‹œëŒ€", "ê·¼ëŒ€"],
            "correct_index": 0,
            "explanation": f"ì´ ìœ ë¬¼ì€ {artifact['period']}ì— ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤."
        }

    def generate_enhanced_explanation(
        self,
        artifact: dict,
        quiz: dict,
        is_correct: bool,
        user_question: str = None
    ) -> str:
        """ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°˜ì˜í•œ ë§ì¶¤ í•´ì„¤ ìƒì„±"""

        base_explanation = quiz.get("explanation", "")

        # ì‚¬ìš©ì ì§ˆë¬¸ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ í•´ì„¤ ë°˜í™˜
        if not user_question or not user_question.strip():
            return base_explanation

        # Gemini APIë¡œ ë§ì¶¤ í•´ì„¤ ìƒì„±
        if self.model:
            try:
                prompt = f"""ë‹¹ì‹ ì€ ë°•ë¬¼ê´€ íë ˆì´í„°ì…ë‹ˆë‹¤.
ì‚¬ìš©ìê°€ ìœ ë¬¼ í€´ì¦ˆë¥¼ í’€ë©´ì„œ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í–ˆìŠµë‹ˆë‹¤.

**ìœ ë¬¼ ì •ë³´:**
- ì´ë¦„: {artifact.get('name', '')}
- ì‹œëŒ€: {artifact.get('period', '')}
- ì§€ì •: {artifact.get('designation', '')}
- ì„¤ëª…: {artifact.get('description', '')}

**í€´ì¦ˆ ë¬¸ì œ:** {quiz.get('question', '')}
**ì •ë‹µ ì—¬ë¶€:** {'ì •ë‹µ' if is_correct else 'ì˜¤ë‹µ'}
**ê¸°ë³¸ í•´ì„¤:** {base_explanation}

**ì‚¬ìš©ìì˜ ê¶ê¸ˆí•œ ì :** {user_question}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ:
1. ë¨¼ì € ê¸°ë³¸ í•´ì„¤ì„ ì œê³µí•˜ê³ 
2. ì‚¬ìš©ìì˜ ê¶ê¸ˆí•œ ì ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”
3. ì¶”ê°€ë¡œ í¥ë¯¸ë¡œìš´ ì •ë³´ê°€ ìˆë‹¤ë©´ ì•Œë ¤ì£¼ì„¸ìš”

ì‘ë‹µì€ 300ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

                response = self.model.generate_content(prompt)
                return response.text
            except Exception as e:
                print(f"ë§ì¶¤ í•´ì„¤ ìƒì„± ì˜¤ë¥˜: {e}")

        # API ì—†ìœ¼ë©´ ê¸°ë³¸ í•´ì„¤ + ì•ˆë‚´ ë©”ì‹œì§€
        return f"""{base_explanation}

ğŸ’¬ **ì§ˆë¬¸í•˜ì‹  ë‚´ìš©:** {user_question}
â†’ API í‚¤ë¥¼ ì„¤ì •í•˜ë©´ ê¶ê¸ˆí•œ ì ì— ëŒ€í•œ ë§ì¶¤ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆì–´ìš”!"""
